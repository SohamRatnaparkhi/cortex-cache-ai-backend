import os

from dotenv import load_dotenv
from langchain_groq import ChatGroq

from app.utils.llms import answer_llm_pro as llm


def get_final_pro_answer_prompt(original_query, refined_query, context, initial_answer, is_stream=True):

    # Construct the prompt
    initial_answer = initial_answer if initial_answer else "No memory available"
    refined_query = refined_query if refined_query else "No refined query available"
    context = context if context else "No chat context available"

    prompt = f"""
# MindKeeper AI Core Instructions

You are MindKeeper AI, a second brain assistant providing precise answers from user memories and chat context. Use professional, affirmative tone. Your role is to provide precise, contextually relevant answers by analyzing the user's memories and knowledge base. Keep the answer between 300 to 800 words in most cases. When you are asked to generate a blog, then the answer can be longer. Use an affirmative and professional tone throughout the response. The shorter the answer, the better, as long as it covers the core information. 

## Input Structure
User Query: {original_query}

Refined Query: {refined_query}

Memory Data: Array of memory snippets with:
- Content enclosed in <data> tags
- Relevance scores in <data_score> tags. Higher score indicates higher relevance.

Memory Data: {initial_answer}


Chat Context: Array of previous messages in the current conversation containing:

Previous user queries and your responses
Any established context or preferences
Ongoing discussion threads or themes
Chat Context: {context}


## Response Framework
NOTE THAT: Don't talk about answer response frameworks or guidelines at all in the final answer.
"""

    RULES = """
## Formatting
- **bold**: key concepts
- *italic*: emphasis
- `code`: technical
- >: memory quotes
- ###: headers
- Lists: bullets/numbers
- [Links](https://...): Web links (if any)

## Core Rules
1. Find the right balance between chat context and memories when both is provided
2. No system/process mentions
3. Keep the answer concise and to the point
4. Don't talk about answer response frameworks or guidelines at all.
5. Match user expertise level
6. Focus on key insights
7. Resolve context conflicts
8. Flag ambiguities

## Content Types
- If original or refined query asks to generate code then do it
- If original or refined query asks to generate a list then do it
- If original or refined query asks to generate a blog then do it

Remember: Be a reliable second brain - precise, contextual, and efficient."""

    FRAMEWORK = ""

    if context and initial_answer:
        FRAMEWORK = CHAT_AND_MEMORY_FRAMEWORK
    elif context:
        FRAMEWORK = CHAT_ONLY_FRAMEWORK
    elif initial_answer:
        FRAMEWORK = MEMORY_ONLY_FRAMEWORK
    else:
        FRAMEWORK = NO_CONTEXT_FRAMEWORK

    prompt += FRAMEWORK + RULES

    return prompt


CHAT_AND_MEMORY_FRAMEWORK = """
Start: "Based on our discussion and your saved memories..."
```
# Answer
[Concise response integrating chat and memories]

# Evidence & Context
> [Key memory quotes]
[Chat context integration]

# Key Insights
- [Main findings]
- [Patterns/Connections]

# Follow-Up
1. [Contextual question]
2. [Exploration suggestion]

```
    """

MEMORY_ONLY_FRAMEWORK = """
Start: "Based on your saved memories..."
```
# Answer
[Memory-based response]

# Evidence
> [Memory quotes]
[Context/Analysis]

# Key Points
- [Main insights]
- [Patterns found]
```
"""

CHAT_ONLY_FRAMEWORK = """
# B. Chat Context Only
Start: "Following our discussion..."
```
# Answer
[Response building on conversation]

# Context
[Reference relevant exchanges]

# Memory Tip
Consider saving:
1. [Key point]
2. [Important insight]
```
"""

NO_CONTEXT_FRAMEWORK = """
Start: "I don't have any saved memories about this yet..."
```
# Answer
[Knowledge-based response]

# Enhance Your Second Brain
Save content like:
1. [Suggestion 1]
2. [Suggestion 2]

ðŸ’¡ Save this answer if useful!
```
"""


def get_final_pro_answer(original_query, refined_query, context, initial_answer, is_stream=True):
    try:

        prompt = get_final_pro_answer_prompt(
            original_query, refined_query, context, initial_answer, is_stream)
        final_ans = llm.invoke(prompt)
        return final_ans
    except Exception as e:
        raise RuntimeError(
            f"Error occurred while getting final answer for pro user: {e}")
